{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import functools \n",
    "import pickle\n",
    "\n",
    "import mmx_lib as mmx\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict, TimeSeriesSplit, KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/analyst/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420584 407490\n"
     ]
    }
   ],
   "source": [
    "#Google-trends\n",
    "gt = pd.read_csv('DATA/google-trends_Trigger-MMX_recent.csv', header=0, index_col=0)\n",
    "#Погода\n",
    "weather = pd.read_csv('DATA/regions_weather.csv', header=0, index_col=0)\n",
    "\n",
    "#Геоданные\n",
    "geo = pd.read_csv('DATA/geo_rus.csv')\n",
    "\n",
    "#Юскан\n",
    "youscan = pd.read_csv('DATA/youscan_Trigger-MMX.csv')\n",
    "youscan = youscan[youscan['fullText'].notna()&youscan['published'].notna()]\n",
    "youscan = youscan.merge(geo[['Город', 'Регион']], left_on='city', right_on='Город', how='left')\n",
    "youscan_no_covid = youscan[~youscan['fullText'].str.contains(r'корона|коронавирус|каронавирус|кароновирус|ковид|covid|corona', case=False,  regex=True)]\n",
    "print(len(youscan), len(youscan_no_covid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Мы хотим, чтобы все индексы были в формате даты, для упрощения объединения и построения графиков\n",
    "weather.index = [datetime.strptime(date+'-0', \"%Y-%W-%w\") for date in weather.index]\n",
    "gt.index = [datetime.strptime(date, \"%Y-%m-%d\") for date in gt.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = youscan_no_covid[['published', 'Регион', 'id']]\n",
    "media = media.groupby(['published', 'Регион'])['id'].count().unstack().fillna(0)\n",
    "media = media[[len(x)==25 for x in media.index]]\n",
    "media.index = [datetime.strptime(date.split('T')[0], \"%Y-%m-%d\") for date in media.index]\n",
    "#Уберем возникшие дупликаты дат\n",
    "media = media.groupby(level=0).sum()\n",
    "\n",
    "#Сгруппировать до недель?\n",
    "media_week = media.copy()\n",
    "media_week['week'] = media_week.index.map(lambda x: datetime.strftime(x, '%Y-%W'))\n",
    "media_week = media_week.groupby('week').sum()\n",
    "media_week.index = [datetime.strptime(date+'-0', \"%Y-%W-%w\") for date in media_week.index]\n",
    "media_week = media_week.groupby(level=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для моделирования отрежем 2020 год\n",
    "this_year = datetime.fromisoformat('2020-01-01')\n",
    "weather = weather[weather.index<this_year]\n",
    "gt = gt[gt.index<this_year]\n",
    "media = media[media.index<this_year]\n",
    "\n",
    "#А это было для сравнения с остальными данными, \n",
    "#которых дальше, чем до 2017 просто нет\n",
    "first_year = datetime.fromisoformat('2017-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наиболее коррелириующие погодные метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5255e5939e1c4ae29930aa1ae317e3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=76.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Выявление зависимостей между погодными метриками и youscan\n",
    "weather_long_corr = {}\n",
    "for reg in tqdm(weather['Регион'].unique()):\n",
    "    w = weather[weather['Регион']==reg].drop(columns=['Регион'])\n",
    "    if reg in list(media_week.columns):\n",
    "        weather_long_corr[reg] = {}\n",
    "        for metric in w.columns:\n",
    "            s1 = w[metric].groupby(level=0).mean()\n",
    "            s2 = media_week[reg].groupby(level=0).sum()\n",
    "            weather_long_corr[reg][metric] = mmx.long_term_correlation(s1, s2, 2, 2, 10)\n",
    "        weather_long_corr[reg] = pd.Series(weather_long_corr[reg])\n",
    "    elif reg in list(gt.columns):\n",
    "        weather_long_corr[reg] = {}\n",
    "        for metric in w.columns:\n",
    "            s1 = w[metric].groupby(level=0).mean()\n",
    "            s2 = gt[reg].groupby(level=0).sum()\n",
    "            weather_long_corr[reg][metric] = mmx.long_term_correlation(s1, s2, 2, 2, 10)\n",
    "        weather_long_corr[reg] = pd.Series(weather_long_corr[reg])\n",
    "        \n",
    "weather_long_corr = pd.DataFrame(weather_long_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Поиск самой похожей погодной метрики\n",
    "metric_dict = {}\n",
    "for reg in weather['Регион'].unique():\n",
    "    if (reg in list(media.columns))|(reg in list(gt.columns)):\n",
    "        metric_max = weather_long_corr[reg].idxmax()\n",
    "        if weather_long_corr.loc[metric_max, reg]>0.1:\n",
    "            metric_dict[reg] = metric_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем двухнедельный прогноз отличается от недельного? Да в общем-то особо ничем. Только тем, на сколько строится предсказание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Значимость признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_list = ['Камчатский край', 'Сахалинская область']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def maximum_search(df, alpha, weeks, period=1):\n",
    "    #сравним текущее значение со средними\n",
    "    nearby_median_long = df.rolling(360, min_periods=30).median()\n",
    "    nearby_median_short = df.rolling(90, min_periods=7).median()\n",
    "    df_delta = ((df - nearby_median_long)/nearby_median_long>alpha)|((df - nearby_median_short)/nearby_median_short>alpha)\n",
    "\n",
    "    #мы хотим отслеживать моменты, когда значения не слишком высоки, но тем не менее стабильно растут\n",
    "    df_diff = df.diff()\n",
    "    df_growing = ((df_diff>0).rolling(7*period).sum()==7*period)&((df - nearby_median_long)/nearby_median_long>0)\n",
    "    df_delta = df_delta|df_growing\n",
    "\n",
    "    #мы готовы допускать пропуски не более, чем в три дня (чем определено?)\n",
    "    df_holes = mmx.find_range_of_holes(df_delta, (1,4))\n",
    "    df_delta = df_delta|df_holes\n",
    "\n",
    "    #мы не хотим пики длиной меньше недели\n",
    "    df_short = mmx.find_range_of_holes(~df_delta, (1, (7*period)+1))\n",
    "    df_delta = df_delta&(~df_short)\n",
    "\n",
    "    #подсчет производных                \n",
    "    df_diff = df_diff.rolling(window=7*period).mean()                \n",
    "    df_diff = mmx.series_norm(df_diff)\n",
    "    df_diff2 = df_diff.diff().rolling(window=7*period).mean()\n",
    "    df_diff2 = mmx.series_norm(df_diff2)\n",
    "\n",
    "    df_week = df.rolling(window=7*period).mean()/nearby_median_short\n",
    "\n",
    "    #Почему так расставлена перменная weeks?\n",
    "    #Потому что для двухнедельного прогноза мы хотим знать, \n",
    "    #что к концу второй недели начнется подъем активности, \n",
    "    #нас интересует именно конец этого диапазона\n",
    "\n",
    "    df_target = (df_delta.shift(-7*period*weeks).rolling(window=7*period).sum()>3*period).astype(int)\n",
    "    return (df_target, df_delta, df_diff, df_diff2, df_week)\n",
    "\n",
    "def region_tuning_weights(weeks=1, num=5):\n",
    "\n",
    "    region_scores = {}\n",
    "    predictions = {}\n",
    "    main_feature = {}\n",
    "    \n",
    "    for reg in tqdm(sorted(list(geo['Регион'].unique()))):\n",
    "#    for reg in tqdm(['Сахалинская область']):\n",
    "        region_scores[reg] = np.nan\n",
    "        predictions[reg] = np.nan\n",
    "        main_feature[reg] = np.nan\n",
    "        media_fullness = np.nan\n",
    "        \n",
    "        flag = True\n",
    "        m_flag = True\n",
    "       \n",
    "        if (reg in list(media.columns))&(reg not in g_list):\n",
    "            \n",
    "        \n",
    "            m = media[reg]\n",
    "            #Проверим средний объем данных (не учитывая этот год, так как он, во-первых, все поломал,\n",
    "            #а, во-вторых, модели обучались на данных без учета 2020 года)\n",
    "            this_year = datetime.fromisoformat('2020-01-01')\n",
    "            m_check = m[m.index<this_year]\n",
    "            m_check = m_check.sum()/len(m_check)\n",
    "            #0.1 - экспериментально полученная величина, при которой еще можно построить предсказание\n",
    "            if m_check>0.1:\n",
    "                #Проверка пройдень, модель строится на медийных данных, флаг переключился\n",
    "                m_flag = False\n",
    "                #Подгрузим остальные данные, если они есть\n",
    "                if reg in list(gt.columns):\n",
    "                    g = gt[reg]\n",
    "                else:\n",
    "                    g = pd.Series()\n",
    "\n",
    "                if (reg in list(weather['Регион']))&(reg in list(metric_dict.keys())):\n",
    "                    w = weather[weather['Регион']==reg][metric_dict[reg]]\n",
    "                    w = w.groupby(level=0).mean()\n",
    "                else:\n",
    "                    w = pd.Series()\n",
    "\n",
    "                #Мы не хотим учитывать слишком большие всплески.\n",
    "                std = m.std()\n",
    "                mean = m.mean()\n",
    "                m = m.apply(lambda x: min(x, mean+3*std))\n",
    "\n",
    "                #Несмотря на то, что мы берем разбивку по дням, лучше данные усреднить\n",
    "                m = m.rolling(window=14).mean()\n",
    "                #А также все отнормировать\n",
    "                m = mmx.series_norm(m)\n",
    "                w = mmx.series_norm(w)\n",
    "                g = mmx.series_norm(g)\n",
    "                #На всякий случай, при выгрузке иногда случаются дупликаты дат\n",
    "                g = g.groupby(level=0).mean()                                    \n",
    "\n",
    "                alpha = 0.3\n",
    "                #Для соцмедиа данных и для google trends посчитаем всякие полезные признаки\n",
    "                m_target, m_delta, m_diff, m_diff2, m_week = maximum_search(m, alpha, weeks, period=1)\n",
    "                g_target, g_delta, g_diff, g_diff2, g_week = maximum_search(g, alpha, weeks, period=2)\n",
    "                #А если данных в регионе маловато, добавим к медийной целевой перменной gt\n",
    "                #if (m_check<1)&(reg in list(gt.columns)): \n",
    "                if (m_check<1):                    \n",
    "                    m_target = (m_target+g_target)>0\n",
    "                    m = m+g\n",
    "                    m = mmx.series_norm(m)\n",
    "                #Мы хотим собрать такой датасет только из того, что действительно есть                   \n",
    "                if (reg in list(weather['Регион']))&(reg in list(metric_dict.keys()))&(reg in list(gt.columns)):\n",
    "                    x = pd.concat([m_week, m_diff<0, m_diff>0, m_diff2, m_delta, \\\n",
    "                                   w.shift(1), \\\n",
    "                                   g_week, g_diff<0, g_diff>0, g_diff2, g_delta], axis=1)\n",
    "                    feature_names = ['mweek', 'mdiff<0', 'mdiff>0', 'mdiff2', 'mdelta', \\\n",
    "                                     'weather', \\\n",
    "                                     'gweek', 'gdiff<0', 'gdiff>0', 'gdiff2', 'gdelta']\n",
    "                elif reg in list(gt.columns):\n",
    "                    x = pd.concat([m_week, m_diff<0, m_diff>0, m_diff2, m_delta, \\\n",
    "                                   g_week, g_diff<0, g_diff>0, g_diff2, g_delta], axis=1)\n",
    "                    feature_names = ['mweek', 'mdiff<0', 'mdiff>0', 'mdiff2', 'mdelta', \\\n",
    "                                    'gweek', 'gdiff<0', 'gdiff>0', 'gdiff2', 'gdelta']\n",
    "                elif (reg in list(weather['Регион']))&(reg in list(metric_dict.keys())):\n",
    "                    x = pd.concat([m_week, m_diff<0, m_diff>0, m_diff2, m_delta, w.shift(1)], axis=1)\n",
    "                    feature_names = ['mweek', 'mdiff<0', 'mdiff>0', 'mdiff2', 'mdelta', \\\n",
    "                                     'weather']\n",
    "                else:\n",
    "                    x = pd.concat([m_week, m_diff<0, m_diff>0, m_diff2, m_delta], axis=1)\n",
    "                    feature_names = ['mweek', 'mdiff<0', 'mdiff>0', 'mdiff2', 'mdelta']\n",
    "\n",
    "        #А вот что, если соцмедиа данных не хватает?\n",
    "        if m_flag:\n",
    "            #Пробуем строить на основе погоды и gt\n",
    "            if ((reg in list(gt.columns))&((reg in list(weather['Регион']))&(reg in list(metric_dict.keys())))&m_flag)|(reg in g_list):\n",
    "                w = weather[weather['Регион']==reg][metric_dict[reg]]\n",
    "                w = w.groupby(level=0).mean()            \n",
    "                w = mmx.series_norm(w)\n",
    "                g = gt[reg]\n",
    "                g = mmx.series_norm(g)\n",
    "\n",
    "                alpha = 0.3\n",
    "                g_target, g_delta, g_diff, g_diff2, g_week = maximum_search(g, alpha, weeks, period=1)\n",
    "                x = pd.concat([w, w.rolling(4).mean(), w.diff(), g_delta, g_week, g_diff<0, g_diff>0, g_diff2], axis=1)\n",
    "                feature_names = ['weather', 'wmean', 'wdiff', 'gdelta', 'gweek', 'gdiff<0', 'gdiff>0', 'gdiff2']\n",
    "                #тогда целевой тпеременно будет только Gt\n",
    "                m_target = g_target.copy()\n",
    "                #и притворимся, что gt - это медиаданные \n",
    "                #(это нужно для корректного выведения данных в дэшборде)\n",
    "                m = g.copy()\n",
    "            else:\n",
    "                flag=False\n",
    "\n",
    "        if flag:\n",
    "            x.columns = feature_names\n",
    "            x = x.replace([np.inf, -np.inf], np.nan)\n",
    "            x = x.fillna(method='ffill')\n",
    "            x = x.astype(float).dropna()\n",
    "            x = (x[x.index.isin(m_target.index)]).astype(float).dropna()\n",
    "            y = m_target.copy()\n",
    "            y = y[y.index.isin(x.index)]\n",
    "\n",
    "            #Для каждого региона модель нужно настраивать отдельно, есть такое дело.\n",
    "            estimators = {\n",
    "                'rf': (RandomForestClassifier(), \n",
    "                       {'n_estimators': [100, 120, 140, 160, 180, 200], 'max_depth': [5, 10, 15, 20]}),\n",
    "                'svc': (SVC(C=1), \n",
    "                        {'gamma': ['scale', 'auto', 2]}),\n",
    "                'gb': ( GradientBoostingClassifier(), \n",
    "                       {'n_estimators': [500, 1000, 2000, 3000, 5000]})                    \n",
    "            }\n",
    "            \n",
    "            try:\n",
    "\n",
    "                params = {}\n",
    "                y_pred_gs = []\n",
    "                for est in estimators:\n",
    "                    clf = estimators[est][0]\n",
    "                    params_dict = estimators[est][1]\n",
    "                    gs = GridSearchCV(clf, params_dict, scoring=mmx.delay_score)\n",
    "                    gs.fit(x, y)\n",
    "                    y_pred_gs.append(gs.predict(x))\n",
    "                    params[est] = gs.best_params_\n",
    "\n",
    "                final_params = {\n",
    "                    'n_estimators': [100, 120, 140, 160, 180, 200],\n",
    "                    'max_depth': [5, 10, 15, 20]\n",
    "                }                \n",
    "                clf = RandomForestRegressor()\n",
    "                gs = GridSearchCV(clf, final_params, scoring=mmx.delay_score)\n",
    "                gs.fit(pd.DataFrame(y_pred_gs).T, y)\n",
    "                final_params = gs.best_params_\n",
    "\n",
    "                n_splits = 10\n",
    "                kf = KFold(n_splits=n_splits, random_state=None, shuffle=False)\n",
    "\n",
    "                #Построение предсказаний\n",
    "                y_pred_total = []\n",
    "                feat_seed = [0]*len(x.columns)\n",
    "                feat_weights = dict(zip(list(estimators.keys()), [feat_seed]*len(estimators)))\n",
    "                final_regr_weight = [0]*len(estimators)\n",
    "                est_exception = dict(zip(list(estimators.keys()), [False]*len(estimators)))\n",
    "                #Кросс-валидация на нересекающихся неперемшанных промежутках\n",
    "                #Почему это важно и почему можно так?\n",
    "                #Потому что наш прогноз основан на данных за последнюю пару недель и нехорошо их совсем перемешивать\n",
    "                #С другой стороны, сама целевая переменная не используется в качестве признаков,\n",
    "                #Так что можно не опасаться использовать в обучении данные из будущего - они никак не помешают настоящему\n",
    "                for train_index, test_index in kf.split(x):\n",
    "                    X_train, X_test = x.iloc[train_index, :], x.iloc[test_index, :]\n",
    "                    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                    #заготовим перменные под промеуточные результаты\n",
    "                    y_pred_i = []\n",
    "                    y_train_i = []\n",
    "                    for est in estimators:\n",
    "                        clf = estimators[est][0].set_params(**params[est])\n",
    "                        clf.fit(X_train, y_train)\n",
    "                        #Так же мы хотим оценивать веса признаков в моделях для разных регионов\n",
    "                        try:\n",
    "                            feat_weights[est] = [x+y for x, y in zip(feat_weights[est], clf.feature_importances_)]\n",
    "                        except:\n",
    "                            est_exception[est] = True\n",
    "                        y_train_i.append(clf.predict(X_train))\n",
    "                        y_pred_i.append(clf.predict(X_test))                    \n",
    "\n",
    "                    regr_total = RandomForestRegressor().set_params(**final_params)                          \n",
    "                    regr_total.fit(pd.DataFrame(y_train_i).T, \n",
    "                                   y_train)\n",
    "                    final_regr_weight = [x+y for x, y in zip(final_regr_weight, regr_total.feature_importances_)]\n",
    "                    y_pred_total.append(pd.Series(\n",
    "                        regr_total.predict(pd.DataFrame(y_pred_i).T), \n",
    "                        index=X_test.index))\n",
    "\n",
    "                feat_weights = {est: [val/n_splits for val in val_list] for est, val_list in feat_weights.items()}\n",
    "                final_regr_weight = [val/n_splits for val in final_regr_weight]\n",
    "                print(final_regr_weight)\n",
    "                for i, est in enumerate(feat_weights.keys()):\n",
    "                    feat_weights[est] = [w*final_regr_weight[i] for w in feat_weights[est]]\n",
    "                feat_weights_final = [sum(i) for i in zip(*list(feat_weights.values()))]\n",
    "                y_pred = pd.concat(y_pred_total).sort_index() \n",
    "\n",
    "                y_int = (y_pred>0.5).astype(int)\n",
    "                y_int = y_int[y_int.index.isin(m_target.index)]\n",
    "                m_target = m_target[m_target.index.isin(y_int.index)]\n",
    "\n",
    "                region_scores[reg] = mmx.delay_func(y_int, m_target)\n",
    "                predictions[reg] = pd.concat([y_pred, y], axis=1)\n",
    "                predictions[reg].columns = ['y_pred', 'y']\n",
    "                main_feature[reg] = dict(zip(feature_names, feat_weights_final))\n",
    "            except:\n",
    "                print(reg, sys.exc_info()[:2])\n",
    "\n",
    "    for est in est_exception:\n",
    "        if est_exception[est]:\n",
    "            print(f'Для {est} нет возможности вычислить значимость признаков')\n",
    "            \n",
    "        \n",
    "    return region_scores, predictions, main_feature, media_fullness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c5ff67fb6e4844985df0eeae7dbd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6db92d2583b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#region_scores, predictions, main_feature, media_fullness = region_tuning_weights(weeks=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mregion_scores_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_feature_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedia_fullness_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregion_tuning_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweeks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-21312877abf9>\u001b[0m in \u001b[0;36mregion_tuning_weights\u001b[0;34m(weeks, num)\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mparams_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                     \u001b[0my_pred_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         n_stages = self._fit_stages(\n\u001b[1;32m   1545\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1621\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m                 \u001b[0;31m# no need to fancy index w/ no subsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_score_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_gb_losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y, raw_predictions, sample_weight)\u001b[0m\n\u001b[1;32m    608\u001b[0m             return (-2 / sample_weight.sum() * np.sum(\n\u001b[1;32m    609\u001b[0m                 sample_weight * ((y * raw_predictions) -\n\u001b[0;32m--> 610\u001b[0;31m                                  np.logaddexp(0, raw_predictions))))\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnegative_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#region_scores, predictions, main_feature, media_fullness = region_tuning_weights(weeks=1)\n",
    "region_scores_2, predictions_2, main_feature_2, media_fullness_2 = region_tuning_weights(weeks=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построить картиночки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение и сохранение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metric_dict.pickle', 'wb') as f:\n",
    "                        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "                        pickle.dump(metric_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_list = ['Камчатский край', 'Сахалинская область']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_search(df, alpha, weeks, period=1):\n",
    "    #сравним текущее значение со средними\n",
    "    nearby_median_long = df.rolling(360, min_periods=30).median()\n",
    "    nearby_median_short = df.rolling(90, min_periods=7).median()\n",
    "    df_delta = ((df - nearby_median_long)/nearby_median_long>alpha)|((df - nearby_median_short)/nearby_median_short>alpha)\n",
    "\n",
    "    #мы хотим отслеживать моменты, когда значения не слишком высоки, но тем не менее стабильно растут\n",
    "    df_diff = df.diff()\n",
    "    df_growing = ((df_diff>0).rolling(7*period).sum()==7*period)&((df - nearby_median_long)/nearby_median_long>0)\n",
    "    df_delta = df_delta|df_growing\n",
    "\n",
    "    #мы готовы допускать пропуски не более, чем в три дня (чем определено?)\n",
    "    df_holes = mmx.find_range_of_holes(df_delta, (1,4))\n",
    "    df_delta = df_delta|df_holes\n",
    "\n",
    "    #мы не хотим пики длиной меньше недели\n",
    "    df_short = mmx.find_range_of_holes(~df_delta, (1, (7*period)+1))\n",
    "    df_delta = df_delta&(~df_short)\n",
    "\n",
    "    #подсчет производных                \n",
    "    df_diff = df_diff.rolling(window=7*period).mean()                \n",
    "    df_diff = mmx.series_norm(df_diff)\n",
    "    df_diff2 = df_diff.diff().rolling(window=7*period).mean()\n",
    "    df_diff2 = mmx.series_norm(df_diff2)\n",
    "\n",
    "    df_week = df.rolling(window=7*period).mean()/nearby_median_short\n",
    "\n",
    "    #Почему так расставлена перменная weeks?\n",
    "    #Потому что для двухнедельного прогноза мы хотим знать, \n",
    "    #что к концу второй недели начнется подъем активности, \n",
    "    #нас интересует именно конец этого диапазона\n",
    "\n",
    "    df_target = (df_delta.shift(-7*period*weeks).rolling(window=7*period).sum()>3*period).astype(int)\n",
    "    return (df_target, df_delta, df_diff, df_diff2, df_week)\n",
    "    \n",
    "    \n",
    "def region_save_models(weeks=1, num=5):\n",
    "    region_scores = {}\n",
    "    predictions = {}\n",
    "    main_feature = {}\n",
    "    \n",
    "    for reg in tqdm(sorted(list(geo['Регион'].unique()))):\n",
    "#     for reg in tqdm(['Московская область', 'Пензенская область', 'Тверская область', 'Хабаровский край']):\n",
    "        region_scores[reg] = np.nan\n",
    "        predictions[reg] = np.nan\n",
    "        main_feature[reg] = np.nan\n",
    "        media_fullness = np.nan\n",
    "        \n",
    "        flag = True\n",
    "        m_flag = True\n",
    "       \n",
    "        if (reg in list(media.columns))&(reg not in g_list):\n",
    "            \n",
    "        \n",
    "            m = media[reg]\n",
    "            #Проверим средний объем данных (не учитывая этот год, так как он, во-первых, все поломал,\n",
    "            #а, во-вторых, модели обучались на данных без учета 2020 года)\n",
    "            this_year = datetime.fromisoformat('2020-01-01')\n",
    "            m_check = m[m.index<this_year]\n",
    "            m_check = m_check.sum()/len(m_check)\n",
    "            #0.1 - экспериментально полученная величина, при которой еще можно построить предсказание\n",
    "            if m_check>0.1:\n",
    "                #Проверка пройдень, модель строится на медийных данных, флаг переключился\n",
    "                m_flag = False\n",
    "                #Подгрузим остальные данные, если они есть\n",
    "                if reg in list(gt.columns):\n",
    "                    g = gt[reg]\n",
    "                else:\n",
    "                    g = pd.Series()\n",
    "\n",
    "                if (reg in list(weather['Регион']))&(reg in list(metric_dict.keys())):\n",
    "                    w = weather[weather['Регион']==reg][metric_dict[reg]]\n",
    "                    w = w.groupby(level=0).mean()\n",
    "                else:\n",
    "                    w = pd.Series()\n",
    "\n",
    "                #Мы не хотим учитывать слишком большие всплески.\n",
    "                std = m.std()\n",
    "                mean = m.mean()\n",
    "                m = m.apply(lambda x: min(x, mean+3*std))\n",
    "\n",
    "                #Несмотря на то, что мы берем разбивку по дням, лучше данные усреднить\n",
    "                m = m.rolling(window=14).mean()\n",
    "                #А также все отнормировать\n",
    "                m = mmx.series_norm(m)\n",
    "                w = mmx.series_norm(w)\n",
    "                g = mmx.series_norm(g)\n",
    "                #На всякий случай, при выгрузке иногда случаются дупликаты дат\n",
    "                g = g.groupby(level=0).mean()                                    \n",
    "\n",
    "                alpha = 0.3\n",
    "                #Для соцмедиа данных и для google trends посчитаем всякие полезные признаки\n",
    "                m_target, m_delta, m_diff, m_diff2, m_week = maximum_search(m, alpha, weeks, period=1)\n",
    "                g_target, g_delta, g_diff, g_diff2, g_week = maximum_search(g, alpha, weeks, period=2)\n",
    "                #А если данных в регионе маловато, добавим к медийной целевой перменной gt\n",
    "                #if (m_check<1)&(reg in list(gt.columns)): \n",
    "                if (m_check<1):                    \n",
    "                    m_target = (m_target+g_target)>0\n",
    "                    m = m+g\n",
    "                    m = mmx.series_norm(m)\n",
    "                #Мы хотим собрать такой датасет только из того, что действительно есть                   \n",
    "                if (reg in list(weather['Регион']))&(reg in list(metric_dict.keys()))&(reg in list(gt.columns)):\n",
    "                    x = pd.concat([m_week, m_diff<0, m_diff>0, m_diff2, m_delta, \\\n",
    "                                   w.shift(1), \\\n",
    "                                   g_week, g_diff<0, g_diff>0, g_diff2, g_delta], axis=1)\n",
    "                    feature_names = ['mweek', 'mdiff<0', 'mdiff>0', 'mdiff2', 'mdelta', \\\n",
    "                                     'weather', \\\n",
    "                                     'gweek', 'gdiff<0', 'gdiff>0', 'gdiff2', 'gdelta']\n",
    "                elif reg in list(gt.columns):\n",
    "                    x = pd.concat([m_week, m_diff<0, m_diff>0, m_diff2, m_delta, \\\n",
    "                                   g_week, g_diff<0, g_diff>0, g_diff2, g_delta], axis=1)\n",
    "                    feature_names = ['mweek', 'mdiff<0', 'mdiff>0', 'mdiff2', 'mdelta', \\\n",
    "                                    'gweek', 'gdiff<0', 'gdiff>0', 'gdiff2', 'gdelta']\n",
    "                elif (reg in list(weather['Регион']))&(reg in list(metric_dict.keys())):\n",
    "                    x = pd.concat([m_week, m_diff<0, m_diff>0, m_diff2, m_delta, w.shift(1)], axis=1)\n",
    "                    feature_names = ['mweek', 'mdiff<0', 'mdiff>0', 'mdiff2', 'mdelta', \\\n",
    "                                     'weather']\n",
    "                else:\n",
    "                    x = pd.concat([m_week, m_diff<0, m_diff>0, m_diff2, m_delta], axis=1)\n",
    "                    feature_names = ['mweek', 'mdiff<0', 'mdiff>0', 'mdiff2', 'mdelta']\n",
    "\n",
    "        #А вот что, если соцмедиа данных не хватает?\n",
    "        if m_flag:\n",
    "            #Пробуем строить на основе погоды и gt\n",
    "            if ((reg in list(gt.columns))&((reg in list(weather['Регион']))&(reg in list(metric_dict.keys())))&m_flag)|(reg in g_list):\n",
    "                w = weather[weather['Регион']==reg][metric_dict[reg]]\n",
    "                w = w.groupby(level=0).mean()            \n",
    "                w = mmx.series_norm(w)\n",
    "                g = gt[reg]\n",
    "                g = mmx.series_norm(g)\n",
    "\n",
    "                alpha = 0.3\n",
    "                g_target, g_delta, g_diff, g_diff2, g_week = maximum_search(g, alpha, weeks, period=1)\n",
    "                x = pd.concat([w, w.rolling(4).mean(), w.diff(), g_delta, g_week, g_diff<0, g_diff>0, g_diff2], axis=1)\n",
    "                feature_names = ['weather', 'wmean', 'wdiff', 'gdelta', 'gweek', 'gdiff<0', 'gdiff>0', 'gdiff2']\n",
    "                #тогда целевой тпеременно будет только Gt\n",
    "                m_target = g_target.copy()\n",
    "                #и притворимся, что gt - это медиаданные \n",
    "                #(это нужно для корректного выведения данных в дэшборде)\n",
    "                m = g.copy()\n",
    "            else:\n",
    "                flag=False\n",
    "\n",
    "        #Так, если у нас ничего не сломалось, то строим предсказание\n",
    "        if flag:  \n",
    "            x.columns = feature_names\n",
    "            x = x.replace([np.inf, -np.inf], np.nan)\n",
    "            x = x.fillna(method='ffill')\n",
    "            x = x.astype(float).dropna()\n",
    "            #Севастополь - особенный город... \n",
    "            #у него до 16 октября 2018 года почему-то беда с данными\n",
    "            #на всякий случай уберем их руками\n",
    "            if reg=='Севастополь':\n",
    "                last_year = datetime.fromisoformat('2018-10-16')\n",
    "                x = x[x.index>last_year]\n",
    "\n",
    "            m_target = m_target.groupby(level=0).mean()\n",
    "            x = x.groupby(level=0).mean()\n",
    "            y = m_target.copy()                \n",
    "            y = y[y.index.isin(x.index)]\n",
    "            x = x[x.index.isin(y.index)]\n",
    "\n",
    "            #Для каждого региона модель нужно настраивать отдельно, есть такое дело.\n",
    "            estimators = {\n",
    "                'rf': [RandomForestClassifier(), \n",
    "                       {'n_estimators': [100, 120, 140, 160, 180, 200], 'max_depth': [5, 10, 15, 20]}],\n",
    "                'svc': [SVC(C=1), \n",
    "                        {'gamma': ['scale', 'auto', 2]}],\n",
    "                'gb': [GradientBoostingClassifier(), \n",
    "                       {'n_estimators': [500, 1000, 2000, 3000, 5000]}]                    \n",
    "            }\n",
    "            #Подблор параметров для ансамбля моделей\n",
    "            try:\n",
    "                params = {}\n",
    "                y_pred_gs = []\n",
    "                for est in estimators:\n",
    "                    clf = estimators[est][0]\n",
    "                    params_dict = estimators[est][1]\n",
    "                    gs = GridSearchCV(clf, params_dict, scoring=mmx.delay_score)\n",
    "                    gs.fit(x, y)\n",
    "                    y_pred_gs.append(gs.predict(x))\n",
    "                    params[est] = gs.best_params_\n",
    "                #подбор параметров для итогового регрессора\n",
    "                final_params = {\n",
    "                    'n_estimators': [100, 120, 140, 160, 180, 200],\n",
    "                    'max_depth': [5, 10, 15, 20]\n",
    "                }                \n",
    "                clf = RandomForestRegressor()\n",
    "                gs = GridSearchCV(clf, final_params, scoring=mmx.delay_score)\n",
    "                gs.fit(pd.DataFrame(y_pred_gs).T, y)\n",
    "                final_params = gs.best_params_\n",
    "                for est in estimators:\n",
    "                        estimators[est][0] = estimators[est][0].set_params(**params[est])\n",
    "                regr_total = RandomForestRegressor().set_params(**final_params)\n",
    "\n",
    "                #Обучение выбранных моделей\n",
    "                y_est = []\n",
    "                est_names = sorted(list(estimators.keys()))\n",
    "                for est in est_names:\n",
    "                    clf = estimators[est][0]\n",
    "                    clf.fit(x, y)\n",
    "                    estimators[est][0] = clf\n",
    "                    y_est.append(clf.predict(x))\n",
    "                    #сохраним модели в pickle\n",
    "                    with open(f'models/testing/{reg}_{est}_{weeks}week.pickle', 'wb') as f:\n",
    "                        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "                        pickle.dump(clf, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                regr_total.fit(pd.DataFrame(y_est).T, y)\n",
    "                with open(f'models/testing/{reg}_finalregr_{weeks}week.pickle', 'wb') as f:\n",
    "                    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "                    pickle.dump(regr_total, f, pickle.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "                with open(f'models/testing/{reg}_colnames_{weeks}week.pickle', 'wb') as f:\n",
    "                    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "                    pickle.dump(feature_names, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except:\n",
    "                print(reg, sys.exc_info()[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3983b943837a498481d369b5193e3998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Якутия (<class 'ValueError'>, ValueError('The number of classes has to be greater than one; got 1 class'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "region_save_models(weeks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91ae4d48d404b05914bdecb6dc110be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Якутия (<class 'ValueError'>, ValueError('The number of classes has to be greater than one; got 1 class'))\n"
     ]
    }
   ],
   "source": [
    "region_save_models(weeks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
